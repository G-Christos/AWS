{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6666efe5",
   "metadata": {},
   "source": [
    "### AWS - Amazon Web Services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079ade4",
   "metadata": {},
   "source": [
    "### About AWS CLI version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cdca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add credentials to CLI\n",
    "aws configure --profile christos\n",
    "aws configure --profile talend_dev\n",
    "aws configure --profile talend_prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9cd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to log into AWS SSO from the command line\n",
    "# https://www.youtube.com/watch?v=YzNX_YZHPXk\n",
    "aws configure sso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c9bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696aa2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default\n"
     ]
    }
   ],
   "source": [
    "!aws configure list-profiles\n",
    "# default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# get the Amazon Resource Name (ARN) of an IAM user\n",
    "# !aws iam list-users --query 'Users[?UserName==`christos`].Arn' --output text\n",
    "# Create an IAM client\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "# Get the IAM user ARN\n",
    "response = iam_client.get_user(UserName='christos')\n",
    "user_arn = response['User']['Arn']\n",
    "print(user_arn) # arn:aws:iam::111111:user/christos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ba7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check current AWS CLI profile is using root or IAM credentials:\n",
    "# query AWS ARN (Amazon Resource Name)\n",
    "!aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146987dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List IAM Roles\n",
    "# !aws iam list-roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give access to IAM user via IAM > Users > ...\n",
    "# Policy name: IAM_access_sqs\n",
    "# go to --> https://awspolicygen.s3.amazonaws.com/policygen.html\n",
    "# Principal: *\n",
    "# Give your permissions!\n",
    "# paste arn of service you want to give access\n",
    "# arn:aws:sqs:eu-central-1:1111:quality_sqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92361ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # original\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Id\": \"__default_policy_ID\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"owner_statement\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": \"arn:aws:iam::111111:root\"\n",
    "      },\n",
    "      \"Action\": \"SQS:*\",\n",
    "      \"Resource\": \"arn:aws:sqs:eu-central-1:111111:quality_sqs\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da71bbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an IAM client\n",
    "# !aws iam list-attached-user-policies --user-name christos\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "# List attached policies for the IAM user\n",
    "response = iam_client.list_attached_user_policies(UserName='christos')\n",
    "\n",
    "# Access the list of policies\n",
    "policies = response['AttachedPolicies']\n",
    "policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24243985",
   "metadata": {},
   "source": [
    "### S3 - SImple Storage Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6e74af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3\n",
    "import boto3, os\n",
    "\n",
    "# Create a session with your credentials\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name= aws_region # 'eu-central-1'\n",
    ")\n",
    "\n",
    "# Create an S3 client\n",
    "# aws_region = 'eu-west-3' # eu-west-3: Paris, 'eu-central-1': Frankfurt\n",
    "s3_client = boto3.client('s3', region_name=aws_region) \n",
    "location = {'LocationConstraint': aws_region}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41f01bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTotal buckets: 7\u001b[0m\n",
      "christos-folder1\n",
      "christos-folder1-backup\n",
      "christos-lambda-functions\n",
      "christos-sqs-lambda-logs\n",
      "dev-l0\n",
      "dev-l1\n",
      "dev-raw\n"
     ]
    }
   ],
   "source": [
    "# List all buckets\n",
    "response = s3_client.list_buckets()\n",
    "\n",
    "# Print the bucket names\n",
    "if 'Buckets' in response:\n",
    "    print('\\033[1m' + \"Total buckets: {}\".format(len(response['Buckets'])) + '\\033[0m') # make a string bold\n",
    "    for bucket in response['Buckets']:\n",
    "        print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef76863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the S3 buckets\n",
    "s3_client.create_bucket(Bucket='dev-raw', CreateBucketConfiguration=location)\n",
    "s3_client.create_bucket(Bucket='dev-l0', CreateBucketConfiguration=location)\n",
    "s3_client.create_bucket(Bucket='dev-l1', CreateBucketConfiguration=location)\n",
    "print(\"Amazon S3 buckets has been created\")  \n",
    "\n",
    "# Define the folder structure (prefixes)\n",
    "folders = ['CE10000', 'KNA1', 'KNVV', 'MARA', 'TVAK']\n",
    "\n",
    "# Create the folders inside the \"dev-raw\" bucket\n",
    "for folder in folders:\n",
    "    key = f'SAP/{folder}/'\n",
    "    s3_client.put_object(Bucket='dev-raw', Key=key)\n",
    "    # print(\"Amazon S3 folder {} has been created inside 'dev-raw' Bucket\".format(key))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the S3 buckets and folders (prefixes) using the AWS CLI\n",
    "# Create the S3 buckets:\n",
    "aws s3api create-bucket --bucket dev-raw --region your-region\n",
    "aws s3api create-bucket --bucket dev-l0 --region your-region\n",
    "aws s3api create-bucket --bucket dev-l1 --region your-region\n",
    "\n",
    "# Create the folder structure (prefixes) inside the \"dev-raw\" bucket:\n",
    "aws s3api put-object --bucket dev-raw --key SAP/CE10000/\n",
    "aws s3api put-object --bucket dev-raw --key SAP/KNA1/\n",
    "aws s3api put-object --bucket dev-raw --key SAP/KNVV/\n",
    "aws s3api put-object --bucket dev-raw --key SAP/MARA/\n",
    "aws s3api put-object --bucket dev-raw --key SAP/TVAK/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22f1a788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:51:07\n"
     ]
    }
   ],
   "source": [
    "# Generate a random time between 00:00:00 and 23:59:59\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_random_time():\n",
    "    # Define the start and end times\n",
    "    start_time = datetime.strptime('00:00:00', '%H:%M:%S')\n",
    "    end_time = datetime.strptime('23:59:59', '%H:%M:%S')\n",
    "\n",
    "    # Calculate the time range in seconds\n",
    "    time_range = (end_time - start_time).total_seconds()\n",
    "\n",
    "    # Generate a random number of seconds within the time range\n",
    "    random_seconds = random.randint(0, int(time_range))\n",
    "\n",
    "    # Add the random number of seconds to the start time\n",
    "    random_time = start_time + timedelta(seconds=random_seconds)\n",
    "\n",
    "    return random_time.time()\n",
    "\n",
    "# Generate a random time\n",
    "random_time = generate_random_time()\n",
    "print(random_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1653f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# Create the folder/prefixes structure inside the \"dev-raw/SAP/CE10000\" bucket for each date & add a csv file.\n",
    "import random, string\n",
    "from datetime import datetime, timedelta\n",
    "folders = ['CE10000', 'KNA1', 'KNVV', 'MARA', 'TVAK']\n",
    "location = {'LocationConstraint': aws_region}\n",
    "n_csv = 0\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Loop for the last 30 days\n",
    "for i in range(30):\n",
    "    # Calculate the date for each iteration\n",
    "    date = current_date - timedelta(days=i)\n",
    "    year = date.strftime(\"%Y\")\n",
    "    month = date.strftime(\"%m\")\n",
    "    day = date.strftime(\"%d\")\n",
    "    \n",
    "    # Create the folder/prefixes structure inside the \"dev-raw\" bucket for each date\n",
    "    for subfolder in folders:\n",
    "        folder = f\"SAP/{subfolder}/EDP_PART_YEAR={year}/EDP_PART_MONTH={month}/EDP_PART_DAY={day}\"\n",
    "        s3_client.put_object(Bucket='dev-raw', Key=folder + '/')\n",
    "        \n",
    "        # Generate a random number of CSV files between 1 and 3\n",
    "        for j in range(random.randint(1, 3)):\n",
    "            # Generate a random timestamp\n",
    "            ts = f\"{year}{month}{day}{generate_random_time()}\".replace(\":\",\"\")       \n",
    "            timestamp_folder = f\"{folder}/EDP_PART_TS={ts}\"\n",
    "            s3_client.put_object(Bucket='dev-raw', Key=timestamp_folder + '/')\n",
    "\n",
    "            # Create a CSV file inside the 'EDP_PART_TS' folder\n",
    "            csv_filename = f\"EDP_MKT_{ts}.csv\"\n",
    "            csv_key = f\"{timestamp_folder}/{csv_filename}\"\n",
    "            s3_client.put_object(Bucket='dev-raw', Key=csv_key)\n",
    "            n_csv += 1\n",
    "            # print(f\"Created CSV file: {csv_key}\") # csv_key, csv_filename\n",
    "            print(csv_key) # csv_key, csv_filename   \n",
    "print(\"Subfolders & {} CSV files created successfully!\".format(n_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "799fb15a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['EDP_MKT_20230615132930.csv',\n",
       " 'EDP_MKT_20230615160848.csv',\n",
       " 'EDP_MKT_20230615234626.csv',\n",
       " 'EDP_MKT_20230615093743.csv',\n",
       " 'EDP_MKT_20230615233535.csv',\n",
       " 'EDP_MKT_20230615093538.csv',\n",
       " 'EDP_MKT_20230615231329.csv',\n",
       " 'EDP_MKT_20230615035259.csv',\n",
       " 'EDP_MKT_20230615053649.csv',\n",
       " 'EDP_MKT_20230615173503.csv',\n",
       " 'EDP_MKT_20230615005254.csv',\n",
       " 'EDP_MKT_20230615110712.csv',\n",
       " 'EDP_MKT_20230615201722.csv']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filenames of all CSV files inside the dev-raw/SAP/ bucket from a specific date\n",
    "from datetime import datetime, timedelta\n",
    "# Specify the bucket name and prefix\n",
    "bucket_name = 'dev-raw'\n",
    "prefix = 'SAP/'\n",
    "\n",
    "# Specify and format the target date as 'YYYYMMDD' for comparison\n",
    "target_date = datetime(2023, 6, 15).strftime('%Y%m%d')\n",
    "\n",
    "# List objects in the bucket with the specified prefix\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# Filter the filenames based on the target date\n",
    "filtered_files = [obj['Key'].rsplit(\"/\",1)[1] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv') and target_date in obj['Key']]\n",
    "print(len(filtered_files))\n",
    "filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd4b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filenames of all CSV files inside the dev-raw/SAP/ bucket from the previous month or a specific date\n",
    "# Specify the bucket name and prefix\n",
    "bucket_name = 'dev-raw'\n",
    "prefix = 'SAP/'\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Calculate the previous month or specify a specific date\n",
    "previous_month = current_date - timedelta(days=current_date.day)\n",
    "specific_date = datetime(2023, 5, 15)  # Example: May 15, 2023\n",
    "\n",
    "# Define the target date\n",
    "target_date = specific_date # Use previous_month or specific_date\n",
    "\n",
    "# Format the target date as 'YYYYMM' for comparison\n",
    "target_date_formatted = target_date.strftime('%Y%m')\n",
    "\n",
    "# List objects in the bucket with the specified prefix\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# Get the filenames of all CSV files in the directory\n",
    "csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]\n",
    "\n",
    "# Filter the filenames based on the target date\n",
    "filtered_files = [file for file in csv_files if target_date_formatted in file] # whole folder path\n",
    "# filtered_files = [file.rsplit(\"/\",1)[1] for file in csv_files if target_date_formatted in file] # just the csv file\n",
    "filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e298244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backups: cp: copy, mv: move & sync: synchronize\n",
    "# aws s3 cp s3://source-bucket/source-folder/ s3://destination-bucket/destination-folder/timestamp/ --recursive\n",
    "!aws s3 cp s3://dev-raw/SAP/CE10000/ s3://christos-lambda-functions/Backup/23230706/SAP/CE10000/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b28a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create backups: cp: copy, mv: move & sync: synchronize\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# Create S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Copy files to destination folder\n",
    "source_bucket = 'dev-raw'\n",
    "source_folder = 'SAP/CE10000'\n",
    "destination_bucket = 'christos-lambda-functions'\n",
    "destination_folder = f'Backup/{timestamp}/SAP/CE10000/'\n",
    "\n",
    "response = s3_client.copy(\n",
    "    {\n",
    "        'Bucket': source_bucket,\n",
    "        'Key': source_folder\n",
    "    },\n",
    "    destination_bucket,\n",
    "    destination_folder,\n",
    "    ExtraArgs={'ACL': 'bucket-owner-full-control'},\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "092b8e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy operation completed.\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create backups: cp: copy, mv: move & sync: synchronize\n",
    "import boto3\n",
    "import datetime\n",
    "\n",
    "# Specify the source and destination bucket names and folder paths\n",
    "source_bucket = 'dev-raw'\n",
    "source_folder = 'SAP/CE10000/'\n",
    "destination_bucket = 'christos-lambda-functions'\n",
    "destination_folder = f'Backup/{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}/SAP/CE10000/'\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Copy the objects from the source folder to the destination folder\n",
    "response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=source_folder)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        key = obj['Key']\n",
    "        destination_key = key.replace(source_folder, destination_folder)\n",
    "        s3_client.copy_object(Bucket=destination_bucket, Key=destination_key, CopySource={'Bucket': source_bucket, 'Key': key})\n",
    "else:\n",
    "    print(\"No objects found in the source folder.\")\n",
    "\n",
    "print(\"Copy operation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_bucket(bucket_name):\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "    except s3_client.exceptions.BucketAlreadyOwnedByYou:\n",
    "        print(f\"Bucket '{bucket_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a3254",
   "metadata": {},
   "source": [
    "### SQS - Simple Queue Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dec3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all Amazon SQS queues and their attributes\n",
    "# Create an SQS client\n",
    "sqs_client = boto3.client('sqs', region_name='eu-central-1') # Frankfurt\n",
    "\n",
    "# List all queues\n",
    "response = sqs_client.list_queues()\n",
    "\n",
    "# Check if the 'QueueUrls' key exists in the response\n",
    "if 'QueueUrls' in response:\n",
    "    # Get the URLs of all queues\n",
    "    queue_urls = response['QueueUrls']\n",
    "    \n",
    "    # Retrieve attributes for each queue\n",
    "    for queue_url in queue_urls:\n",
    "        response = sqs_client.get_queue_attributes(QueueUrl=queue_url, AttributeNames=['All'])\n",
    "        attributes = response['Attributes']\n",
    "        print(f\"Queue URL: {queue_url}\")\n",
    "        # print(f\"Attributes: {attributes}\\n\")\n",
    "else:\n",
    "    print(\"No queues found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589c6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all Amazon SQS queues\n",
    "#!aws sqs list-queues --region eu-central-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83913c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"MD5OfMessageBody\": \"f0e606842daa91e42994fd5be9a27e90\",\n",
      "    \"MessageId\": \"08de730a-f9f8-4f5d-8fb0-df06bc8cfcdc\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create a new sqs event using cli ! I use Paris region as default for CLI --prifile\n",
    "!aws sqs send-message --queue-url https://sqs.eu-central-1.amazonaws.com/1111111/quality_sqs --message-body EDP_MAT_20230707094800.csv --region eu-central-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a New SQS event\n",
    "### Existed Queue ###\n",
    "# Create an SQS client\n",
    "region_name = 'eu-central-1' # 'eu-west-3' 'eu-central-1' # Specify the region\n",
    "sqs_client = boto3.client('sqs', region_name=region_name)\n",
    "\n",
    "# Get the URL of the existing queue\n",
    "queue_name = 'quality_sqs'\n",
    "response = sqs_client.get_queue_url(QueueName=queue_name)\n",
    "queue_url = response['QueueUrl']\n",
    "\n",
    "# import json\n",
    "# csv = {\"csv\":\"EDP_MAT_20230629125300.csv\"}\n",
    "# message_body = json.dumps({'csv': 'EDP_MAT_20230629135600.csv'})\n",
    "message_body = 'EDP_MAT_20230706214800.csv'\n",
    "\n",
    "response = sqs_client.send_message(QueueUrl=queue_url, MessageBody=message_body)\n",
    "print(\"Message sent. Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Existed Queue ###\n",
    "# Create an SQS client\n",
    "region_name = 'eu-central-1' # 'eu-west-3' 'eu-central-1' # Specify the region\n",
    "sqs_client = boto3.client('sqs', region_name=region_name)\n",
    "\n",
    "# Get the URL of the existing queue\n",
    "queue_name = 'quality_sqs'\n",
    "response = sqs_client.get_queue_url(QueueName=queue_name)\n",
    "queue_url = response['QueueUrl']\n",
    "\n",
    "# Get the attributes of the existing queue\n",
    "response = sqs_client.get_queue_attributes(QueueUrl=queue_url, AttributeNames=['All'])\n",
    "attributes = response['Attributes']\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4479e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### New SQS dl Queue ###\n",
    "# Create a new queue with the same settings\n",
    "response = sqs_client.create_queue(QueueName=\"data_quality_dl_sqs\")\n",
    "# Get the URL of the new queue\n",
    "new_queue_url = response['QueueUrl']\n",
    "\n",
    "# Set configurable attributes for the new queue\n",
    "sqs_client.set_queue_attributes(\n",
    "    QueueUrl=new_queue_url,\n",
    "    Attributes={\n",
    "        'DelaySeconds': '0',\n",
    "        #'MaximumMessageSize': '262144',\n",
    "        'VisibilityTimeout': '30',\n",
    "        # Add other configurable attributes as needed\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"New queue URL: {new_queue_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### New SQS Queue ###\n",
    "# Create a new queue with the same settings\n",
    "response = sqs_client.create_queue(QueueName=\"data_quality_sqs\")\n",
    "# Get the URL of the new queue\n",
    "new_queue_url = response['QueueUrl']\n",
    "\n",
    "# Set configurable attributes for the new queue\n",
    "sqs_client.set_queue_attributes(\n",
    "    QueueUrl=new_queue_url,\n",
    "    Attributes={\n",
    "        'DelaySeconds': '0',\n",
    "        #'MaximumMessageSize': '262144',\n",
    "        'VisibilityTimeout': '30',\n",
    "        'RedrivePolicy': '{\"deadLetterTargetArn\":\"arn:aws:sqs:eu-central-1:111111:data_quality_dl_sqs\",\"maxReceiveCount\":3}'\n",
    "        # Add other configurable attributes as needed\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"New queue URL: {new_queue_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL of the existing queue\n",
    "queue_name = 'data_quality_sqs'\n",
    "response = sqs_client.get_queue_url(QueueName=queue_name)\n",
    "queue_url = response['QueueUrl']\n",
    "\n",
    "# Get the attributes of the existing queue\n",
    "response = sqs_client.get_queue_attributes(QueueUrl=queue_url, AttributeNames=['All'])\n",
    "attributes = response['Attributes']\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56408feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n"
     ]
    }
   ],
   "source": [
    "# To view the details of an SQS message based on its MessageId\n",
    "!aws sqs receive-message --queue-url https://sqs.eu-central-1.amazonaws.com/491362005797/quality_sqs --message-attribute-names All --attribute-names All --max-number-of-messages 1 --wait-time-seconds 0 --query \"Messages[?MessageId=='7ff88a01-57ad-40c9-9096-c164cb897842']\" --region eu-central-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64d2d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view the details of an SQS queue\n",
    "!aws sqs receive-message --queue-url https://sqs.eu-central-1.amazonaws.com/491362005797/quality_sqs --max-number-of-messages 5 --region eu-central-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d357ae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No messages found\n"
     ]
    }
   ],
   "source": [
    "# Specify the MessageId of the message you want to retrieve\n",
    "message_id = \"7455a244-6ab6-4ab1-a4f9-8cd9a934af6c\"\n",
    "\n",
    "# Retrieve the message using its MessageId\n",
    "response = sqs_client.receive_message(\n",
    "    QueueUrl=queue_url,\n",
    "    AttributeNames=['All'],\n",
    "    MessageAttributeNames=['All'],\n",
    "    MaxNumberOfMessages=1,\n",
    "    WaitTimeSeconds=0,\n",
    "    ReceiveRequestAttemptId='string'\n",
    ")\n",
    "\n",
    "# Check if the response contains any messages\n",
    "if 'Messages' in response:\n",
    "    messages = response['Messages']\n",
    "    for message in messages:\n",
    "        # Check if the MessageId matches the desired one\n",
    "        if message['MessageId'] == message_id:\n",
    "            # Print the message details\n",
    "            print(\"MessageId:\", message['MessageId'])\n",
    "            print(\"Body:\", message['Body'])\n",
    "            # Other message attributes and metadata can be accessed in a similar way\n",
    "else:\n",
    "    print(\"No messages found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a9e03",
   "metadata": {},
   "source": [
    "### lambda_handler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42fc55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event: \"EDP_MAT_20230706214800.csv\"\n",
    "# Start Time: 2023-07-06 18:47:41\n",
    "# Execution Time: 0:00:00.000211 seconds\n",
    "# Data quality check: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ccec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H_%M_%S\") # time.time()\n",
    "start_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") # time.time()\n",
    "start_time = datetime.datetime.now() # .replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import datetime\n",
    "# from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Process the SQS event\n",
    "    for record in event['Records']:\n",
    "        # Extract the message body from the event record\n",
    "        message_body = record['body']\n",
    "        \n",
    "        # Process the message body\n",
    "        process_message(message_body)\n",
    "    \n",
    "    # # Return a response\n",
    "    # return {\n",
    "    #     'statusCode': 200,\n",
    "    #     'event': record,\n",
    "    #     'body': 'SQS event processed successfully'\n",
    "    # }\n",
    "\n",
    "def process_message(message_body):\n",
    "    # Process the message body\n",
    "    print(f\"Received message: {message_body}\")\n",
    "    # Add your logic here to handle the message\n",
    "    event = message_body\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"start_time: {}\".format(start_time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "    # Print the event\n",
    "    print(\"event: {}\".format(event))\n",
    "\n",
    "    # Save the log file locally\n",
    "    ts = start_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    object_key = f\"{event}_{ts}.txt\"\n",
    "    log_file_name = f\"/tmp/{object_key}\"\n",
    "\n",
    "    st = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(log_file_name, 'w') as file:\n",
    "        file.write(f\"Event: {json.dumps(event)}\\n\")\n",
    "        file.write(f\"Start Time: {st}\\n\")\n",
    "\n",
    "    # Perform your other logic here\n",
    "    # extra code here!\n",
    "    import random\n",
    "    # Generate a random variable (0 or 1)\n",
    "    random_var = random.randint(0, 1)\n",
    "    \n",
    "    # Check the value of the random variable and print the corresponding message\n",
    "    if random_var == 0:\n",
    "        print(\"Data quality check: identical\", random_var)\n",
    "    else:\n",
    "        print(\"Data quality check: Not Identical\", random_var)   \n",
    "    \n",
    "    # Calculate execution time\n",
    "    execution_time = datetime.datetime.now() - start_time\n",
    "\n",
    "    # # Append execution time to the log file\n",
    "    with open(log_file_name, 'a') as file:\n",
    "        file.write(f\"Execution Time: {execution_time} seconds\\n\")\n",
    "        file.write(f\"Data quality check: {random_var}\\n\")\n",
    "        \n",
    "    # Upload the log file to S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket_name = 'christos-sqs-lambda-logs'\n",
    "    s3_client.upload_file(log_file_name, bucket_name, object_key)\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(event)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c37e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Process the SQS event\n",
    "    for record in event['Records']:\n",
    "        # Extract the message body from the event record\n",
    "        message_body = record['body']\n",
    "        \n",
    "        # Process the message body\n",
    "        process_message(message_body)\n",
    "    \n",
    "    # Return a response\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'SQS event processed successfully'\n",
    "    }\n",
    "\n",
    "def process_message(message_body):\n",
    "    # Process the message body\n",
    "    print(f\"Received message: {message_body}\")\n",
    "    # Add your logic here to handle the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ff34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# def lambda_handler(event, context):\n",
    "#     # TODO implement\n",
    "#     print(event)\n",
    "#     # csv = event[\"csv\"]\n",
    "#     # print(csv)\n",
    "#     import json\n",
    "#     return {\n",
    "#         'statusCode': 200,\n",
    "#         'body': json.dumps(event)\n",
    "#     }\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import datetime\n",
    "# import time\n",
    "    \n",
    "def lambda_handler(event, context):\n",
    "    # Process the SQS event\n",
    "    for record in event['Records']:\n",
    "        # Extract the message body from the event record\n",
    "        message_body = record['body']\n",
    "        \n",
    "        # Process the message body\n",
    "        process_message(message_body)\n",
    "    \n",
    "    # # Return a response\n",
    "    # return {\n",
    "    #     'statusCode': 200,\n",
    "    #     'event': record,\n",
    "    #     'body': 'SQS event processed successfully'\n",
    "    # }\n",
    "\n",
    "def process_message(message_body):\n",
    "    # Process the message body\n",
    "    print(f\"Received message: {message_body}\")\n",
    "    # Add your logic here to handle the message\n",
    "    event = message_body\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"start_time: {}\".format(start_time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "    # Print the event\n",
    "    print(\"event: {}\".format(event))\n",
    "\n",
    "    # Save the log file locally\n",
    "    ts = start_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    object_key = f\"{event}_{ts}.txt\"\n",
    "    log_file_name = f\"/tmp/{object_key}\"\n",
    "\n",
    "    st = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(log_file_name, 'w') as file:\n",
    "        file.write(f\"Event: {json.dumps(event)}\\n\")\n",
    "        file.write(f\"Start Time: {st}\\n\")\n",
    "\n",
    "    # Perform your other logic here\n",
    "    # extra code here!\n",
    "    import random\n",
    "    # Generate a random variable (0 or 1)\n",
    "    random_var = random.randint(0, 1)\n",
    "    \n",
    "    # Check the value of the random variable and print the corresponding message\n",
    "    if random_var == 0:\n",
    "        print(\"Data quality check: identical\", random_var)\n",
    "    else:\n",
    "        print(\"Data quality check: Not Identical\", random_var)   \n",
    "    \n",
    "    # Calculate execution time\n",
    "    execution_time = datetime.datetime.now() - start_time\n",
    "\n",
    "    # # Append execution time to the log file\n",
    "    with open(log_file_name, 'a') as file:\n",
    "        file.write(f\"Execution Time: {execution_time} seconds\\n\")\n",
    "        file.write(f\"Data quality check: {random_var}\\n\")\n",
    "        \n",
    "    # Upload the log file to S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket_name = 'christos-sqs-lambda-logs'\n",
    "    s3_client.upload_file(log_file_name, bucket_name, object_key)\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(event),\n",
    "        \"Not Identical:\": random_var\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fab18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the Lambda function permission to write to the /tmp directory and upload files to an S3 bucket\n",
    "import boto3\n",
    "# Create an IAM client\n",
    "iam_client = boto3.client('iam', region_name='eu-central-1') #   region_name=region_name \n",
    "\n",
    "# Create the IAM role\n",
    "role_name = 'LambdaS3AccessRole_logs'\n",
    "role_policy_document = {\n",
    "    'Version': '2012-10-17',\n",
    "    'Statement': [\n",
    "        {\n",
    "            'Effect': 'Allow',\n",
    "            'Principal': {\n",
    "                'Service': 'lambda.amazonaws.com'\n",
    "            },\n",
    "            'Action': 'sts:AssumeRole'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "response = iam_client.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps(role_policy_document)\n",
    ")\n",
    "\n",
    "# Attach the policies to the role\n",
    "role_arn = response['Role']['Arn']\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    ")\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess'\n",
    ")\n",
    "\n",
    "# Update the Lambda function's execution role\n",
    "lambda_client = boto3.client('lambda', region_name='eu-central-1')\n",
    "function_name = 'DataQualityCheck'\n",
    "lambda_client.update_function_configuration(\n",
    "    FunctionName=function_name,\n",
    "    Role=role_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08850d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Process each record in the event\n",
    "    for record in event['Records']: # event['Records'][0]['body']\n",
    "        # Retrieve the message body from the record\n",
    "        message_body = record['body']\n",
    "        \n",
    "        # Deserialize the message body into a Python dictionary\n",
    "        message_data = json.loads(message_body)\n",
    "        \n",
    "        # Access the CSV filename from the message data\n",
    "        csv_filename = message_data['csv']\n",
    "        \n",
    "        # Perform any necessary processing with the CSV filename\n",
    "        print(f\"Received CSV filename: {csv_filename}\")\n",
    "        \n",
    "    # Return a response, if needed\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'Message processed successfully',\n",
    "        'csv' : csv_filename\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a new Lambda function\n",
    "function_name = 'new-lambda-function'  # Replace with the desired name for the new function\n",
    "runtime = 'python3.8'  # Replace with the desired runtime\n",
    "handler = 'lambda_function.handler'  # Replace with the desired handler function\n",
    "role_arn = 'arn:aws:iam::11111111111:role/lambda-role'  # Replace with the ARN of the IAM role for the function\n",
    "timeout = 60  # Replace with the desired timeout in seconds\n",
    "memory_size = 128  # Replace with the desired memory size in megabytes\n",
    "\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName=function_name,\n",
    "    Runtime=runtime,\n",
    "    Role=role_arn,\n",
    "    Handler=handler,\n",
    "    Timeout=timeout,\n",
    "    MemorySize=memory_size,\n",
    "    Publish=True  # Set to True if you want the function to be published\n",
    ")\n",
    "\n",
    "# Optionally, you can print the newly created function's ARN\n",
    "function_arn = response['FunctionArn']\n",
    "print('New Lambda function ARN:', function_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d79ed77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To reproduce a Lambda function from an existing one\n",
    "# # Create a Boto3 client for Lambda\n",
    "# lambda_client = boto3.client('lambda', region_name='eu-central-1')\n",
    "\n",
    "# # Retrieve the existing Lambda function's configuration\n",
    "# function_name = 'DataQualityCheck'  # actual name of the existing function\n",
    "# response = lambda_client.get_function(FunctionName=function_name)\n",
    "# function_configuration = response['Configuration']\n",
    "\n",
    "# # Retrieve the existing Lambda function's code\n",
    "# function_code = lambda_client.get_function(FunctionName=function_name)['Code']\n",
    "\n",
    "# # Create a new Lambda function using the retrieved configuration\n",
    "# new_function_name = 'new-lambda-function'  # Replace with the desired name for the new function\n",
    "# response = lambda_client.create_function(\n",
    "#     FunctionName=new_function_name,\n",
    "#     Runtime=function_configuration['Runtime'],\n",
    "#     Role=function_configuration['Role'],\n",
    "#     Handler=function_configuration['Handler'],\n",
    "#     Description=function_configuration['Description'],\n",
    "#     Timeout=function_configuration['Timeout'],\n",
    "#     MemorySize=function_configuration['MemorySize'],\n",
    "#     Publish=True  # Set to True if you want the function to be published\n",
    "# )\n",
    "\n",
    "# # Set the code for the new Lambda function using the retrieved code\n",
    "# lambda_client.update_function_code(\n",
    "#     FunctionName=new_function_name,\n",
    "#     ZipFile=function_code['Location']\n",
    "# )\n",
    "\n",
    "# print('Lambda function reproduced successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44eea1",
   "metadata": {},
   "source": [
    "### CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b187bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !aws logs filter-log-events --log-group-name /aws/lambda/DataQualityCheck --region eu-central-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575e21b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "# Create a CloudWatch Logs client\n",
    "client = boto3.client('logs', region_name='eu-central-1')\n",
    "\n",
    "# Specify the start time and end time in ISO 8601 format\n",
    "start_time = datetime(2023, 7, 3, 0, 0, 0) #.isoformat()\n",
    "end_time = datetime(2023, 7, 4, 23, 59, 59) #.isoformat()\n",
    "\n",
    "# Retrieve log events for a specific log group\n",
    "response = client.filter_log_events(\n",
    "    logGroupName='/aws/lambda/DataQualityCheck',\n",
    "    startTime=int(start_time.timestamp() * 1000),  # Convert to milliseconds\n",
    "    endTime=int(end_time.timestamp() * 1000)  # Convert to milliseconds\n",
    ")\n",
    "\n",
    "# Process the log events\n",
    "for event in response['events']:\n",
    "    print(event['message'])\n",
    "    if event['message'].startswith(\"REPORT RequestId\"):\n",
    "        print(50*\"-\",\"END\",50*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2f384",
   "metadata": {},
   "source": [
    "### Aurora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c90376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Create an RDS client\n",
    "rds_client = boto3.client('rds', region_name='eu-central-1')\n",
    "\n",
    "# Create an Aurora database cluster\n",
    "rds_client.create_db_cluster(\n",
    "    DBClusterIdentifier='my-db-cluster', # Production template\n",
    "    Engine='aurora', # Aurora Standard, PostgreSQL 14.6\n",
    "    # Specify other cluster configuration parameters\n",
    ") # username: postgres, password: 12345678!\n",
    "# KMS key ID   alias/aws/rds\n",
    "# Tag key: devops-guru-default\n",
    "# Tag value: my-db-cluster\n",
    "\n",
    "# Cost per resource per hour\n",
    "# $0.0042 Amazon DevOps Guru pricing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3346647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Identifier: my-db-cluster\n",
      "Endpoint: my-db-cluster.cluster-c9i3cb16qopx.eu-central-1.rds.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the attributes of an existing Aurora cluster\n",
    "import boto3\n",
    "\n",
    "# Create an RDS client\n",
    "rds_client = boto3.client('rds', region_name='eu-central-1')\n",
    "\n",
    "# Specify the identifier of the cluster you want to describe\n",
    "cluster_identifier = 'my-db-cluster'\n",
    "\n",
    "# Retrieve the cluster attributes\n",
    "response = rds_client.describe_db_clusters(DBClusterIdentifier=cluster_identifier)\n",
    "\n",
    "# Extract the relevant attributes from the response\n",
    "cluster = response['DBClusters'][0]\n",
    "\n",
    "# Print the attributes\n",
    "print(\"Cluster Identifier:\", cluster['DBClusterIdentifier'])\n",
    "print(\"Endpoint:\", cluster['Endpoint'])\n",
    "# ... print other attributes as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c65a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-db-cluster-instance-1\n"
     ]
    }
   ],
   "source": [
    "# Create an RDS client\n",
    "rds_client = boto3.client('rds', region_name='eu-central-1')\n",
    "\n",
    "# Retrieve the details of the Aurora cluster\n",
    "response = client.describe_db_clusters(DBClusterIdentifier=cluster_identifier) # my-db-cluster'\n",
    "\n",
    "# Extract the list of databases from the response\n",
    "databases = response['DBClusters'][0]['DBClusterMembers'][0]['DBInstanceIdentifier']\n",
    "\n",
    "print(databases) # Print the list of databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb75bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a database within the cluster\n",
    "rds_client.create_db_instance(\n",
    "    DBInstanceIdentifier='my-db-instance',\n",
    "    Engine='aurora-postgresql', # not just aurora\n",
    "    DBClusterIdentifier='my-db-cluster',\n",
    "    DBInstanceClass='db.r5.large'  # Example instance class, choose according to your needs\n",
    "    # Specify other database configuration parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table within the database\n",
    "aurora_client = boto3.client('rds-data', region_name='eu-central-1')\n",
    "aurora_client.execute_statement(\n",
    "    secretArn='my-db-secret-arn',\n",
    "    database='qlt',\n",
    "    sql='CREATE TABLE data_quality (csv VARCHAR(255) PRIMARY KEY, time_check TIMESTAMP, duration INTEGER, identical BOOLEAN)',\n",
    "    # Specify other execution parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56900f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Create a Secrets Manager client\n",
    "secrets_manager_client = boto3.client('secretsmanager')\n",
    "\n",
    "# Specify the name or identifier of the secret\n",
    "secret_name = 'my-db-secret'\n",
    "\n",
    "# Retrieve the secret value\n",
    "response = secrets_manager_client.describe_secret(SecretId=secret_name)\n",
    "\n",
    "# Extract the ARN from the response\n",
    "secret_arn = response['ARN']\n",
    "\n",
    "# Print the secret ARN\n",
    "print(f\"Secret ARN: {secret_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"your_host\",\n",
    "    port=\"your_port\",\n",
    "    database=\"your_database\",\n",
    "    user=\"your_user\",\n",
    "    password=\"your_password\"\n",
    ")\n",
    "\n",
    "# Create a cursor to execute SQL queries\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute a SQL query\n",
    "query = \"SELECT * FROM your_table\"\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch the query results\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Lambda function to insert data into the table\n",
    "def lambda_handler(event, context):\n",
    "    # Extract the necessary data from the event\n",
    "    csv = event['csv']\n",
    "    time_check = event['time_check']\n",
    "    duration = event['duration']\n",
    "    identical = event['identical']\n",
    "\n",
    "    # Insert the data into the table\n",
    "    aurora_client.execute_statement(\n",
    "        secretArn='my-db-secret-arn',\n",
    "        database='qlt',\n",
    "        sql='INSERT INTO data_quality (csv, time_check, duration, identical) VALUES (:csv, :time_check, :duration, :identical)',\n",
    "        parameters=[\n",
    "            {'name': 'csv', 'value': {'stringValue': csv}},\n",
    "            {'name': 'time_check', 'value': {'stringValue': str(time_check)}},\n",
    "            {'name': 'duration', 'value': {'longValue': duration}},\n",
    "            {'name': 'identical', 'value': {'booleanValue': identical}},\n",
    "        ],\n",
    "        ...\n",
    "        # Specify other execution parameters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ac4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1e20216",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Define the bucket name and file key\n",
    "bucket_name = 'your_bucket_name'\n",
    "file_key = 'path/to/your/file.csv'\n",
    "\n",
    "# Example 1: Download a file from S3\n",
    "\n",
    "# Specify the local file path to save the downloaded file\n",
    "local_file_path = 'path/to/save/local_file.csv'\n",
    "\n",
    "# Download the file from S3\n",
    "s3_client.download_file(bucket_name, file_key, local_file_path)\n",
    "print('File downloaded successfully.')\n",
    "\n",
    "# Example 2: Upload a file to S3\n",
    "\n",
    "# Specify the local file path of the file to upload\n",
    "local_file_path = 'path/to/your/local_file.csv'\n",
    "\n",
    "# Specify the S3 key for the file\n",
    "s3_file_key = 'path/to/uploaded/file.csv'\n",
    "\n",
    "# Upload the file to S3\n",
    "s3_client.upload_file(local_file_path, bucket_name, s3_file_key)\n",
    "print('File uploaded successfully.')\n",
    "\n",
    "# Example 3: List objects in an S3 bucket\n",
    "\n",
    "# List objects in the bucket\n",
    "response = s3_client.list_objects(Bucket=bucket_name)\n",
    "\n",
    "# Iterate over the objects and print their names\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])\n",
    "\n",
    "# Example 4: Read a file directly from S3 into a pandas DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the S3 object key of the file to read\n",
    "s3_object_key = 'path/to/your/file.csv'\n",
    "\n",
    "# Read the file directly from S3 into a DataFrame\n",
    "df = pd.read_csv(f's3://{bucket_name}/{s3_object_key}')\n",
    "print(df.head())\n",
    "\n",
    "# Example 5: Write a pandas DataFrame to a file in S3\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'Column1': [1, 2, 3], 'Column2': ['A', 'B', 'C']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the S3 object key for the file\n",
    "s3_object_key = 'path/to/write/file.csv'\n",
    "\n",
    "# Write the DataFrame to a file and upload it to S3\n",
    "df.to_csv(f's3://{bucket_name}/{s3_object_key}', index=False)\n",
    "print('DataFrame written and uploaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202601a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "file1_path = 'file1.csv'\n",
    "file2_path = 'file2.csv'\n",
    "\n",
    "# Load the CSV file with tab delimiter\n",
    "df = pd.read_csv(file2_path, delimiter='\\t')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming common_cols is a list of column names\n",
    "common_cols = ['Col1', 'Col2', 'Col3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224cf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame with specified data types\n",
    "df = pd.read_csv('output.csv', dtype={'column1': int, 'column2': float, 'column3': str})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0faee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Change column names\n",
    "new_columns = ['X', 'Y', 'Z']\n",
    "df.columns = new_columns\n",
    "\n",
    "# Display the DataFrame with updated column names\n",
    "print(\"\\nDataFrame with Updated Column Names:\")\n",
    "print(df)\n",
    "\n",
    "# Assuming df is the DataFrame and file_path is the path to the output Excel file\n",
    "# Export DataFrame to Excel\n",
    "df.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa51dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOS!\n",
    "# def lambda_handler(event, context):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 resource\n",
    "s3_resource = session.resource('s3')\n",
    "\n",
    "# Specify the bucket name\n",
    "bucket_name = 'your-bucket-name'\n",
    "\n",
    "# List objects in the bucket\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "for obj in bucket.objects.all():\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the object exists in the bucket\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=file_path)\n",
    "if 'Contents' in response:\n",
    "    print(\"File exists.\")\n",
    "else:\n",
    "    print(\"File does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5b070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['kunnr', 'vkorg', 'vtweg', 'spart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61512f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the bucket name\n",
    "bucket_name = 'my-bucket'\n",
    "\n",
    "# List all objects in the bucket\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Delimiter='/')\n",
    "\n",
    "# Print the \"folder\" names (prefixes)\n",
    "if 'CommonPrefixes' in response:\n",
    "    for prefix in response['CommonPrefixes']:\n",
    "        print(prefix['Prefix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Data to be written to the CSV file\n",
    "data = [\n",
    "    ['John', 'Doe', 30],\n",
    "    ['Jane', 'Smith', 25],\n",
    "    ['Michael', 'Johnson', 35]\n",
    "]\n",
    "\n",
    "# Specify the file path and name for the CSV file\n",
    "output_file = 'output.csv'\n",
    "\n",
    "# Create the CSV file and write the data\n",
    "with open(output_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"CSV file '{output_file}' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09390de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "\n",
    "# Replace colons with underscores in the timestamp\n",
    "filename = timestamp.replace(\":\", \"_\") + \".txt\"\n",
    "\n",
    "# Create and write to the file\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(\"This is a test file.\")\n",
    "\n",
    "print(f\"File '{filename}' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e952f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws lambda create-function --function-name my-function --runtime python3.9 --handler my_script.lambda_handler --role arn:aws:iam::1234567890:role/my-role --code S3Bucket=my-bucket,S3Key=my-script.zip\n",
    "# aws lambda invoke --function-name my-function --payload '{}' output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa063167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can I export an SQS queue to cli script?\n",
    "# aws sqs send-message --queue-url <queue-url> --message-body \"Hello, world!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4fa2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SQS client:\n",
    "# sqs_client = boto3.client('sqs', region_name='your-region-name', aws_access_key_id='your-access-key', aws_secret_access_key='your-secret-access-key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861272eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SQS queue:\n",
    "queue_name = 'your-queue-name'\n",
    "response = sqs_client.create_queue(QueueName=queue_name)\n",
    "queue_url = response['QueueUrl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Lambda function:\n",
    "lambda_client = boto3.client('lambda', region_name='your-region-name', aws_access_key_id='your-access-key', aws_secret_access_key='your-secret-access-key')\n",
    "lambda_function_name = 'your-lambda-function-name'\n",
    "lambda_queue_arn = response['QueueArn']\n",
    "lambda_client.create_event_source_mapping(\n",
    "    EventSourceArn=lambda_queue_arn,\n",
    "    FunctionName=lambda_function_name,\n",
    "    Enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fc8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_body = 'your-message-body'\n",
    "sqs_client.send_message(QueueUrl=queue_url, MessageBody=message_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can I create a IAM user with read only access to a specific S3 bucket via aws cli2?\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"S3ReadAccess\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::your-bucket-name\",\n",
    "                \"arn:aws:s3:::your-bucket-name/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the IAM user: Run the following command to create the IAM user:\n",
    "# aws iam create-user --user-name your-iam-username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5344fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the IAM policy to the user: \n",
    "# aws iam put-user-policy --user-name your-iam-username --policy-name s3-read-only-policy --policy-document file://s3-read-only-policy.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws lambda create-event-source-mapping \\\n",
    "    --function-name your-lambda-function \\\n",
    "    --event-source-arn your-sqs-queue-arn \\\n",
    "    --batch-size 10 \\\n",
    "    --starting-position LATEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqs_client = boto3.client('sqs', region_name='your-region-name')\n",
    "queue_url = 'your-sqs-queue-url'\n",
    "argument = 'your-argument'\n",
    "\n",
    "response = sqs_client.send_message(\n",
    "    QueueUrl=queue_url,\n",
    "    MessageBody=argument\n",
    ")\n",
    "\n",
    "print(\"Message sent:\", response['MessageId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db703373",
   "metadata": {},
   "source": [
    "[AWS Pricing Calculator](https://calculator.aws/) <br>\n",
    "https://calculator.aws/#/addService <br>\n",
    "https://docs.aws.amazon.com/pricing-calculator/latest/userguide/what-is-pricing-calculator.html <br>\n",
    "https://aws.amazon.com/redshift/pricing/ <br>\n",
    "https://aws.amazon.com/s3/pricing/ <br>\n",
    "https://www.cloudysave.com/aws/cost-calculator/s3-cost-calculator/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47915a90",
   "metadata": {},
   "source": [
    "[Spotify Data Pipeline: Extract, Transform, and Analyze with AWS](https://github.com/srikantaghosh/Data-Engineering-Spotify-End-to-End-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add dependencies to your Lambda function\n",
    "lambda_function/\n",
    "     lambda_function.py\n",
    "     dependencies/\n",
    "         package1/\n",
    "         package2/\n",
    "         ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required dependencies inside the \"dependencies\" director\n",
    "pip install pandas -t dependencies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad754217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Set the path to your Lambda function code and dependencies directory\n",
    "function_code_path = 'lambda_function/lambda_function.py'\n",
    "dependencies_dir = 'lambda_function/dependencies/'\n",
    "\n",
    "# Set the path for the deployment package\n",
    "deployment_package_path = 'lambda_function/deployment_package.zip'\n",
    "\n",
    "# Create a new zip file for the deployment package\n",
    "with zipfile.ZipFile(deployment_package_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add the Lambda function code to the zip file\n",
    "    zipf.write(function_code_path, os.path.basename(function_code_path))\n",
    "\n",
    "    # Add the dependencies to the zip file\n",
    "    for root, _, files in os.walk(dependencies_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, os.path.relpath(file_path, dependencies_dir))\n",
    "\n",
    "print('Deployment package created:', deployment_package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f4b56a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   A  B  C\n",
      "0  1  4  7\n",
      "1  2  5  8\n",
      "2  3  6  9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd762de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5fccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
